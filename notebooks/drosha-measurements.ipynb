{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "documentary-straight",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We're now going to take a look at the drosha measurements and how to featurize them onto the graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-feedback",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyprojroot import here\n",
    "import pandas as pd\n",
    "\n",
    "df_bioc = pd.read_csv(here() / \"data/df_bioc.csv\", index_col=0)\n",
    "df_bioc.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-geneva",
   "metadata": {},
   "source": [
    "There are a lot of columns in there, however, the ones we are most interested in are:\n",
    "\n",
    "- `frac_avg`: Gives us the activity\n",
    "- `dot_bracket`: Gives us the dot-bracket notation\n",
    "\n",
    "Things that we may be interested in include:\n",
    "\n",
    "- The `shannon_{pos}` series of columns, which gives us the shannon entropy of that particular position in the folded RNA.\n",
    "\n",
    "Our goal here is to predict `frac_avg` (or some math transform of it) from the `dot_bracket` structure.\n",
    "Our hypothesis here is that the `dot_bracket` structure represented as a graph\n",
    "gives us sufficient information to predict `frac_avg` accurately;\n",
    "alternatively, we might want to add in the shannon entropy,\n",
    "as we found previously that it was visually\\* correlated with RNA cleavage (`frac_avg`).\n",
    "\n",
    "\n",
    "> \\* by visually correlated, we refer to Fig. 2 of [our previously-published paper](https://www.sciencedirect.com/science/article/abs/pii/S1097276520307358)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-europe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from drosha_gnn.graph import to_networkx\n",
    "import janitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-frank",
   "metadata": {},
   "source": [
    "## Make graphs from dot-bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-corporation",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jax.scipy.special import logit\n",
    "import jax.numpy as np\n",
    "\n",
    "def logit_transform(value, tol: float = 1e-5):\n",
    "    return logit(np.clip(value, tol, 1-tol))\n",
    "\n",
    "df = (\n",
    "    df_bioc\n",
    "    .join_apply(lambda row: to_networkx(row[\"dot_bracket\"]), \"graph\")\n",
    "    .transform_column(\"frac_avg\", logit_transform, \"frac_avg_logit\")\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-madness",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "\n",
    "def ecdf(data):\n",
    "    x = np.sort(data)\n",
    "    y = np.arange(len(data)) / (len(data) + 1)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-victim",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x, y = ecdf(df_bioc[\"frac_avg\"].values)\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-canvas",
   "metadata": {},
   "source": [
    "We have a pretty even distribution of points here.\n",
    "\n",
    "One thing that we'll definitely want to do is to regress on the logits,\n",
    "so we'll have to transform the `frac_avg` column to logit space instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-given",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "df_bioc.transform_column(\"frac_avg\", logit_transform, \"frac_avg_logit\")[\"frac_avg_logit\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-lesson",
   "metadata": {},
   "source": [
    "We'll also need to annotate each node on the graph with its nucleotide identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-given",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graphs = df[\"graph\"].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-stuff",
   "metadata": {},
   "source": [
    "Annotate nucleotide and entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-surname",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import networkx as nx \n",
    "\n",
    "def annotate_nucleotide(G: nx.Graph, sequence: str):\n",
    "    nucleotides = sorted(\"AUGC\")\n",
    "    for i, letter in enumerate(sequence):\n",
    "        G.nodes[i][\"nucleotide\"] = letter\n",
    "        G.nodes[i][\"nucleotide_idx\"] = nucleotides.index(letter) + 1\n",
    "    return G\n",
    "\n",
    "def annotate_entropy(G: nx.graph, entropy_vector: np.ndarray):\n",
    "    for node, entropy in zip(G.nodes(), entropy_vector):\n",
    "        G.nodes[node][\"entropy\"] = entropy\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-dakota",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_entropy_vector(df, row):\n",
    "    r = df.loc[row]\n",
    "    entropy_cols = sorted([c for c in df.columns if \"shannon\" in c])\n",
    "    return r[entropy_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-mexican",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx, g in graphs.items():\n",
    "    seq = df.loc[idx][\"seq\"]\n",
    "    g = annotate_nucleotide(g, seq)\n",
    "    \n",
    "    entropy_vec = get_entropy_vector(df, idx)\n",
    "    g = annotate_entropy(g, entropy_vec)\n",
    "    graphs[idx] = g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-policy",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graphs[761].nodes(data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-nicholas",
   "metadata": {},
   "source": [
    "## Transformation to graph data structures\n",
    "\n",
    "We're now going to make the feature matrix and adjacency matrix for each graph.\n",
    "The key here is that we have to pad it to a particular size\n",
    "in order for the operations to work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-conjunction",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jax import jit\n",
    "\n",
    "def prep_feats(F, size):\n",
    "    # F is of shape (n_nodes, n_feats)\n",
    "    return np.pad(\n",
    "        F,\n",
    "        [\n",
    "            (0, size - F.shape[0]),\n",
    "            (0, 0)\n",
    "        ],\n",
    "    )\n",
    "\n",
    "def prep_adjs(A, size):\n",
    "    # A is of shape (n_nodes, n_nodes)\n",
    "    return np.pad(\n",
    "        A,\n",
    "        [\n",
    "            (0, size-A.shape[0]),\n",
    "            (0, size-A.shape[0]),\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-directive",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "largest_graph_size = max(len(g) for g in graphs.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-error",
   "metadata": {},
   "source": [
    "pd.Series(graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-client",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feat_matrix(G):\n",
    "    feats = []\n",
    "    for n, d in G.nodes(data=True):\n",
    "        feat_vect = np.array([d[\"nucleotide_idx\"], d[\"entropy\"]])\n",
    "        feats.append(feat_vect)\n",
    "    feats = np.stack(feats)\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-identity",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "feat_matrices = dict()\n",
    "for idx, graph in tqdm(graphs.items()):\n",
    "    feat_matrices[idx] = prep_feats(feat_matrix(graph), largest_graph_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-following",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adj_matrices = dict()\n",
    "for idx, graph in tqdm(graphs.items()):\n",
    "    adj_matrices[idx] = prep_adjs(np.array(nx.adjacency_matrix(graph).todense()), largest_graph_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-blend",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.Series(adj_matrices, name=\"adj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-office",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.Series(feat_matrices, name=\"feats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-cylinder",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph_matrices = dict()\n",
    "for (idx, adj), (idx2, feat) in zip(adj_matrices.items(), feat_matrices.items()):\n",
    "    graph_matrices[idx] = np.concatenate([adj, feat], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-trunk",
   "metadata": {},
   "source": [
    "Now, we can start designing a graph attention network to do this!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-training",
   "metadata": {},
   "source": [
    "Firstly, we need a node embedding layer. \n",
    "For this, we will borrow inspiration from the language modelling world.\n",
    "Our \"vocabulary\" is the letters \"AUGC\",\n",
    "so we'll use a learnable embedding for each letter.\n",
    "Every node feature vector's first slot is dedicated to an integer value\n",
    "that we can use to index into the embedding vector.\n",
    "We'll make the embedding vector length 256,\n",
    "just for funzies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-editor",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jax import random\n",
    "\n",
    "rng = random.PRNGKey(99)\n",
    "# vocab_size = 4\n",
    "# embedding_size = 256\n",
    "# embedding_matrix = random.normal(rng, shape=(vocab_size, embedding_size))\n",
    "\n",
    "# indices = np.array([0.0, 1.0, 1.0, 3.0, 2.0]).astype(int)\n",
    "# np.take(embedding_matrix, indices, axis=0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-character",
   "metadata": {},
   "source": [
    "When it comes to GNN operations,\n",
    "some involve the feature matrix only,\n",
    "others involve the adjacency matrix only,\n",
    "and yet others involve both the adjacency and feature matrices.\n",
    "\n",
    "To simplify the representation of a graph,\n",
    "let's consider the case where we have only a 2D matrix.\n",
    "It is of size (num_nodes, num_nodes + num_features).\n",
    "What do they semantically mean?\n",
    "\n",
    "- The (num_nodes, num_nodes) portion (left side of the matrix) is the adjacency matrix.\n",
    "- The (num_nodes, num_features) portion (right side of the matrix) is the feature matrix.\n",
    "\n",
    "In each step, we can accept the entire thing as one piece, and then split accordingly.\n",
    "\n",
    "Let's call this matrix the \"graph matrix\"\n",
    "\n",
    "Because one graph is one sample, its shape, then, is defined as `(num_nodes, num_nodes + num_features)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-gravity",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jax import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-beaver",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_feats(graph_mat, num_nodes: int):\n",
    "    return graph_mat[:, num_nodes:]\n",
    "\n",
    "def select_adj(graph_mat, num_nodes: int):\n",
    "    return graph_mat[:, :num_nodes]\n",
    "\n",
    "def RnaGraphEmbedding(num_nodes: int, embedding_size: int):\n",
    "    vocab_size = 4\n",
    "    def init_fun(rng, input_shape):\n",
    "        \"\"\"\n",
    "        :param input_shape: (num_nodes, num_nodes + num_features)\n",
    "        \"\"\"\n",
    "        num_nodes, num_nodes_features = input_shape\n",
    "        num_features = num_nodes_features - num_nodes\n",
    "\n",
    "        embedding_matrix = random.normal(rng, shape=(vocab_size, embedding_size))\n",
    "        # Add a zeros vector to the beginning for padded vector.\n",
    "        embedding_matrix = np.concatenate([np.zeros((1, embedding_size)), embedding_matrix])\n",
    "        return (num_nodes, num_nodes + embedding_size,), embedding_matrix\n",
    "    \n",
    "    def apply_fun(params, inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        :param inputs: The node feature matrix.\n",
    "            We assume that the node feature matrix's first column\n",
    "            is the embedding index.\n",
    "        \"\"\"\n",
    "        embedding_matrix = params\n",
    "        adj = select_adj(inputs, num_nodes)\n",
    "        feats = select_feats(inputs, num_nodes)\n",
    "\n",
    "        indices = np.take(feats, 0, axis=1).astype(int)\n",
    "        embedding = np.take(embedding_matrix, indices, axis=0)\n",
    "        \n",
    "        output = np.concatenate([adj, embedding], axis=1)\n",
    "        return output\n",
    "        \n",
    "    return init_fun, apply_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-logistics",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_fun, apply_fun = RnaGraphEmbedding(num_nodes=170, embedding_size=256)\n",
    "output_shape, params = init_fun(rng, input_shape=(170, 2))\n",
    "\n",
    "out = apply_fun(params, (graph_matrices[763]))\n",
    "# out[0].shape, out[1].shape\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-reservation",
   "metadata": {},
   "source": [
    "We also need a layer that simply extracts out the rest of the node features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-paste",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jax import lax\n",
    "def NodeFeatureExtractor(num_nodes: int):\n",
    "    def init_fun(rng, input_shape):\n",
    "        num_nodes, num_feats = input_shape\n",
    "        return (num_nodes, num_nodes + input_shape[-1] - 1,), ()\n",
    "    \n",
    "    def apply_fun(params, inputs, **kwargs):\n",
    "        adj = select_adj(inputs, num_nodes)\n",
    "        feats = select_feats(inputs, num_nodes)\n",
    "\n",
    "        return np.concatenate([adj, feats[:, 1:]], axis=1)\n",
    "    \n",
    "    return init_fun, apply_fun\n",
    "    \n",
    "    \n",
    "init_fun, apply_fun = NodeFeatureExtractor(num_nodes=170)\n",
    "_, params = init_fun(rng, (170, 2))\n",
    "out = apply_fun(params, (graph_matrices[763]))\n",
    "out.shape, _"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-burton",
   "metadata": {},
   "source": [
    "Now we can do the fan-out operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-arthritis",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jax.experimental import stax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-tiffany",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "node_featurization = stax.serial(\n",
    "    stax.FanOut(2),\n",
    "    stax.parallel(\n",
    "        RnaGraphEmbedding(num_nodes=170, embedding_size=256),\n",
    "        NodeFeatureExtractor(num_nodes=170),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-floor",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test-drive\n",
    "init_fun, apply_fun = node_featurization\n",
    "output_shape, params = init_fun(rng, input_shape=(170, 2,))\n",
    "\n",
    "inputs = apply_fun(params, (graph_matrices[763]))\n",
    "output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-forestry",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jax.tree_util import tree_map\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def GraphFanInConcat(num_nodes: int, axis: int = -1):\n",
    "    def init_fun(rng, input_shape):\n",
    "        \n",
    "        num_feats = np.sum(np.array([i[1] - num_nodes for i in input_shape]))\n",
    "        return (num_nodes, num_nodes + num_feats), ()\n",
    "\n",
    "    def apply_fun(params, inputs, **kwargs):\n",
    "        adj = tree_map(partial(select_adj, num_nodes=170), inputs)\n",
    "        feats = tree_map(partial(select_feats, num_nodes=170), inputs)\n",
    "        feats = np.concatenate(feats, axis=1)\n",
    "        return np.concatenate([adj[0], feats], axis=1)\n",
    "\n",
    "    return init_fun, apply_fun\n",
    "\n",
    "\n",
    "init_fun, apply_fun = stax.serial(\n",
    "    stax.FanOut(2),\n",
    "    stax.parallel(\n",
    "        RnaGraphEmbedding(num_nodes=170, embedding_size=256),\n",
    "        NodeFeatureExtractor(num_nodes=170),\n",
    "    ),\n",
    "    GraphFanInConcat(num_nodes=170)\n",
    ")\n",
    "output_shape, params = init_fun(rng, input_shape=(170, 2))\n",
    "out = apply_fun(params, graph_matrices[763])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-quebec",
   "metadata": {},
   "source": [
    "After that, we do the Graph attention layer.\n",
    "I've written this layer a few times,\n",
    "but I'd like to do this layer in a fashion\n",
    "that makes sense for this problem.\n",
    "\n",
    "The Graph attention layer accepts in a graph matrix.\n",
    "It then computes a node-by-node similarity matrix based on the node information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-december",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from jax import vmap\n",
    "\n",
    "def concat_nodes(node1, node2):\n",
    "    \"\"\"Concatenate two nodes together.\"\"\"\n",
    "    return np.concatenate([node1, node2])\n",
    "\n",
    "\n",
    "def concatenate(node: np.ndarray, node_feats: np.ndarray):\n",
    "    \"\"\"Concatenate node with each node in node_feats.\n",
    "\n",
    "    Behaviour is as follows.\n",
    "    Given a node with features `f_0` and stacked node features\n",
    "    `[f_0, f_1, f_2, ..., f_N]`,\n",
    "    return a stacked concatenated feature array:\n",
    "    `[(f_0, f_0), (f_0, f_1), (f_0, f_2), ..., (f_0, f_N)]`.\n",
    "    \n",
    "    :param node: A vector embedding of a single node in the graph.\n",
    "        Should be of shape (n_input_features,)\n",
    "    :param node_feats: Stacked vector embedding of all nodes in the graph.\n",
    "        Should be of shape (n_nodes, n_input_features)\n",
    "    :returns: A stacked array of concatenated node features.\n",
    "    \"\"\"\n",
    "    return vmap(partial(concat_nodes, node))(node_feats)\n",
    "\n",
    "\n",
    "def concatenate_node_features(node_feats):\n",
    "    \"\"\"Return node-by-node concatenated features.\n",
    "    \n",
    "    Given a node feature matrix of shape (n_nodes, n_features),\n",
    "    this returns a matrix of shape (n_nodes, n_nodes, 2*n_features).\n",
    "    \"\"\"\n",
    "    outputs = vmap(partial(concatenate, node_feats=node_feats))(node_feats)\n",
    "    return outputs\n",
    "\n",
    "outputs = concatenate_node_features(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-portal",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jax import nn\n",
    "\n",
    "def AttentiveMessagePassingLayer(num_nodes: int, hidden_dims: int):\n",
    "    \"\"\"Attentive message passing on a graph.\n",
    "    \n",
    "    We use a feed forward neural network to learn\n",
    "    the weights on which a message passing operator should work.\n",
    "    \n",
    "    The input is the graph matrix. Should be of size (num_nodes, num_nodes + num_feats).\n",
    "    The output is also of the size (num_nodes, num_nodes + num_feats).\n",
    "    \"\"\"\n",
    "\n",
    "    def init_fun(rng, input_shape):\n",
    "        num_nodes, n_node_feats = input_shape\n",
    "        num_feats = n_node_feats - num_nodes\n",
    "        k1, k2, k3, k4 = random.split(rng, 4)\n",
    "        \n",
    "        # Params for neural network transformation of node concatenated features.\n",
    "        w1 = random.normal(k1, shape=(2 * num_feats, hidden_dims)) * 0.001\n",
    "        b1 = random.normal(k2, shape=(hidden_dims,)) * 0.001\n",
    "        w2 = random.normal(k3, shape=(hidden_dims,)) * 0.001\n",
    "        b2 = random.normal(k4, shape=(1,)) * 0.001\n",
    "        \n",
    "        params = w1, b1, w2, b2\n",
    "        output_shape = (num_nodes, num_nodes + num_feats)\n",
    "        return output_shape, params\n",
    "    \n",
    "    def apply_fun(params, inputs, **kwargs):\n",
    "        ### START ATTENTIVE MATRIX CALCULATION ###\n",
    "        w1, b1, w2, b2 = params\n",
    "        adj = select_adj(inputs, num_nodes)\n",
    "        feats = select_feats(inputs, num_nodes)\n",
    "        node_by_node_concat = concatenate_node_features(feats)\n",
    "        \n",
    "        # Neural network piece here.\n",
    "        a1 = nn.relu(np.dot(node_by_node_concat, w1) + b1)\n",
    "        a2 = np.dot(a1, w2) + b2\n",
    "\n",
    "        attentive_adj = adj * a2\n",
    "        ### END ATTENTIVE MATRIX CALCULATION ###\n",
    "        mp = np.dot(attentive_adj, feats)\n",
    "        return np.concatenate([adj, mp], axis=1)\n",
    "\n",
    "    return init_fun, apply_fun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-problem",
   "metadata": {},
   "source": [
    "Now, we need an attentive graph summation layer.\n",
    "\n",
    "If we define attention as just \"sample driven, fancy ways of calculating linear weighting...\",\n",
    "then we can use a neural network to calculate weights per sample.\n",
    "\n",
    "The input is the graph matrix of shape `(num_nodes, num_nodes + num_feats)`.\n",
    "Inside this function, we take the matrix and do a neural network forward pass\n",
    "to produce a vector that is of length `(num_nodes,)`,\n",
    "which we can call the \"attentive weights\".\n",
    "Finally, we take feature matrix portion of the graph matrix\n",
    "and dot product it against the attentive weights\n",
    "to arrive at the summed up vector\n",
    "to give effectively a graph-level vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-iraqi",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def AttentiveGraphSummation(num_nodes, hidden_dims: int = 2048):\n",
    "    def init_fun(rng, input_shape):\n",
    "        num_nodes, num_node_feats = input_shape\n",
    "        num_feats = num_node_feats - num_nodes\n",
    "        \n",
    "        k1, k2, k3, k4 = random.split(rng, 4)\n",
    "        # Params for neural network transformation of node concatenated features.\n",
    "        w1 = random.normal(k1, shape=(num_feats, hidden_dims)) * 0.001\n",
    "        b1 = random.normal(k2, shape=(hidden_dims,)) * 0.001\n",
    "        w2 = random.normal(k3, shape=(hidden_dims, num_nodes)) * 0.001\n",
    "        b2 = random.normal(k4, shape=(num_nodes,)) * 0.001\n",
    "        params = w1, b1, w2, b2\n",
    "        output_shape = (num_feats,)\n",
    "        return output_shape, params\n",
    "    \n",
    "    def apply_fun(params, inputs, **kwargs):\n",
    "        w1, b1, w2, b2 = params\n",
    "        feats = select_feats(inputs, num_nodes)\n",
    "        \n",
    "        # Neural network piece here.\n",
    "        a1 = nn.relu(np.dot(feats, w1) + b1)\n",
    "        a2 = np.tanh(np.dot(a1, w2) + b2)\n",
    "        node_attn_weights = np.tanh(np.sum(a2, axis=0))\n",
    "\n",
    "        # Weighted summation happens here\n",
    "        out = np.dot(node_attn_weights, feats)\n",
    "        return out\n",
    "    return init_fun, apply_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-apollo",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def AttentionEverywhereGNN(num_nodes: int):\n",
    "\n",
    "    init_fun, apply_fun = stax.serial(\n",
    "        stax.FanOut(2),\n",
    "        stax.parallel(\n",
    "            RnaGraphEmbedding(num_nodes=num_nodes, embedding_size=256),\n",
    "            NodeFeatureExtractor(num_nodes=num_nodes),\n",
    "        ),\n",
    "        GraphFanInConcat(num_nodes=num_nodes),\n",
    "        AttentiveMessagePassingLayer(num_nodes=num_nodes, hidden_dims=256),\n",
    "        AttentiveGraphSummation(num_nodes=num_nodes),\n",
    "        stax.Dense(256),\n",
    "        stax.Relu,\n",
    "        stax.Dense(256),\n",
    "        stax.Relu,\n",
    "        stax.Dense(1),\n",
    "    )\n",
    "    \n",
    "    return init_fun, apply_fun\n",
    "\n",
    "init_fun, apply_fun = AttentionEverywhereGNN(170)\n",
    "\n",
    "output_shape, params = init_fun(rng, input_shape=(170, 2))\n",
    "out = apply_fun(params, graph_matrices[763])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-religion",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-rochester",
   "metadata": {},
   "source": [
    "## Train neural network\n",
    "\n",
    "\n",
    "We can now train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-publication",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = np.stack(pd.Series(graph_matrices).values)\n",
    "y = np.stack(df[\"frac_avg_logit\"].values).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-sierra",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_test_split(rng, X, y, train_fraction=0.7):\n",
    "    indices = np.arange(len(X))\n",
    "    indices = random.permutation(rng, indices)\n",
    "    num_train = int(len(X) * train_fraction)\n",
    "    train_idxs = indices[:num_train]\n",
    "    test_idxs = indices[num_train:]\n",
    "    return train_idxs, test_idxs\n",
    "\n",
    "train_idxs, test_idxs = train_test_split(rng, X, y)\n",
    "X_train = X[train_idxs]\n",
    "X_test = X[test_idxs]\n",
    "y_train = y[train_idxs]\n",
    "y_test = y[test_idxs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-rachel",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jax import grad, vmap\n",
    "\n",
    "## Training loop\n",
    "def mse(y_true: np.array, y_pred: np.array):\n",
    "    return np.mean(np.power(y_true - y_pred, 2))\n",
    "\n",
    "\n",
    "def mseloss(params, model, X, y):\n",
    "    \"\"\"MSE loss.\"\"\"\n",
    "    y_pred = vmap(partial(model, params))(X)\n",
    "    return mse(y, y_pred)\n",
    "\n",
    "\n",
    "dmseloss = grad(mseloss)\n",
    "init_fun, model = AttentionEverywhereGNN(170)\n",
    "_, params = init_fun(rng, input_shape=(170, 2))\n",
    "\n",
    "mseloss(params, model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-algeria",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drosha-gnn",
   "language": "python",
   "name": "drosha-gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
